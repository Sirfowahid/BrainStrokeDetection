{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhgE2Fyp7op0goGdiFZic1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sirfowahid/BrainStrokeDetection/blob/main/CGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJCvipLXE6F6",
        "outputId": "30028470-c79c-41d8-c157-c0ffbfa579ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCWNHaa2E1Bz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import Grayscale\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "learning_rate_g = 0.0002\n",
        "learning_rate_d = 0.0001\n",
        "epochs = 60\n",
        "latent_dim = 100\n",
        "num_classes = 2\n",
        "img_size = 224\n",
        "\n",
        "# Transformation: Convert images to grayscale with fixed size\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Load dataset using ImageFolder\n",
        "dataset = datasets.ImageFolder(\n",
        "    root='/content/drive/MyDrive/Projects/23. Brain Stroke Prediction/Dataset',\n",
        "    transform=transform\n",
        ")\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes, img_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + num_classes, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, img_size * img_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_input = self.label_embedding(labels)\n",
        "        gen_input = torch.cat((noise, label_input), -1)\n",
        "        img = self.model(gen_input)\n",
        "        img = img.view(img.size(0), 1, self.img_size, self.img_size)  # Output grayscale image\n",
        "        return img\n",
        "\n",
        "# Define Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_classes, img_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.label_embedding = nn.Embedding(num_classes, img_size * img_size)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(2, 16, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(16, 32, 3, 2, 1),\n",
        "            nn.BatchNorm2d(32, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(32, 64, 3, 2, 1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 3, 2, 1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        ds_size = img_size // 2 ** 4\n",
        "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        label_embedding = self.label_embedding(labels).view(labels.size(0), 1, img.size(2), img.size(3))\n",
        "        d_in = torch.cat((img, label_embedding), 1)\n",
        "        out = self.model(d_in)\n",
        "        out = out.view(out.shape[0], -1)\n",
        "        validity = self.adv_layer(out)\n",
        "        return validity\n",
        "\n",
        "# Initialize models\n",
        "generator = Generator(latent_dim, num_classes, img_size)\n",
        "discriminator = Discriminator(num_classes, img_size)\n",
        "\n",
        "# Loss and Optimizers\n",
        "adversarial_loss = nn.BCELoss()\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate_g, betas=(0.5, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate_d, betas=(0.5, 0.999))\n",
        "\n",
        "# Loss trackers\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "# Function to plot generated images\n",
        "def plot_generated_images(epoch, generator, latent_dim, num_classes):\n",
        "    z = torch.randn(16, latent_dim)\n",
        "    labels = torch.randint(0, num_classes, (16,))\n",
        "    gen_imgs = generator(z, labels).detach().cpu()\n",
        "\n",
        "    fig, axs = plt.subplots(4, 4, figsize=(8, 8))\n",
        "    for i in range(16):\n",
        "        axs[i // 4, i % 4].imshow(gen_imgs[i, 0], cmap='gray')\n",
        "        axs[i // 4, i % 4].axis('off')\n",
        "    plt.suptitle(f'Generated Images at Epoch {epoch + 1}')\n",
        "    plt.show()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    for i, (real_imgs, labels) in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n",
        "        batch_size = real_imgs.size(0)\n",
        "\n",
        "        # Adversarial ground truths with label smoothing\n",
        "        valid = torch.full((batch_size, 1), 0.9)  # Smooth labels for real images\n",
        "        fake = torch.full((batch_size, 1), 0.0)\n",
        "\n",
        "        # Configure input\n",
        "        real_imgs = real_imgs\n",
        "        labels = labels\n",
        "\n",
        "        # Train Generator\n",
        "        optimizer_G.zero_grad()\n",
        "        z = torch.randn(batch_size, latent_dim)\n",
        "        gen_labels = torch.randint(0, num_classes, (batch_size,))\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "        g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid)\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Train Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "        real_loss = adversarial_loss(discriminator(real_imgs, labels), valid)\n",
        "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # Save losses\n",
        "        g_losses.append(g_loss.item())\n",
        "        d_losses.append(d_loss.item())\n",
        "\n",
        "    print(f\"[Epoch {epoch + 1}/{epochs}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
        "\n",
        "    # Plot generated images every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        plot_generated_images(epoch, generator, latent_dim, num_classes)\n",
        "\n",
        "# Plot the losses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(g_losses, label=\"Generator Loss\")\n",
        "plt.plot(d_losses, label=\"Discriminator Loss\")\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torchvision.transforms import ToPILImage\n",
        "\n",
        "def plot_single_generated_image(generator, latent_dim, num_classes):\n",
        "    # Generate a batch of images\n",
        "    z = torch.randn(16, latent_dim)  # Batch size of 16\n",
        "    labels = torch.randint(0, num_classes, (16,))  # Batch size of 16\n",
        "\n",
        "    # Generate images from the batch\n",
        "    with torch.no_grad():\n",
        "        gen_imgs = generator(z, labels).detach().cpu()\n",
        "\n",
        "    # Extract a single image from the batch\n",
        "    single_img = gen_imgs[0]  # Take the first image in the batch\n",
        "\n",
        "\n",
        "    # Remove batch dimension if needed\n",
        "    if single_img.dim() == 4:  # Check if it has batch and channel dimensions\n",
        "        single_img = single_img.squeeze(0)  # Remove batch dimension\n",
        "    if single_img.size(0) == 1:  # Check if image is grayscale\n",
        "        single_img = single_img.expand(3, -1, -1)  # Convert grayscale to RGB\n",
        "\n",
        "    # Resize image to 224x224 if necessary\n",
        "    single_img = torch.nn.functional.interpolate(single_img.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False).squeeze(0)\n",
        "\n",
        "    # Convert tensor to PIL image\n",
        "    to_pil = ToPILImage()\n",
        "    img = to_pil(single_img)\n",
        "\n",
        "    # Plot the single image\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Generated Image')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "latent_dim = 100\n",
        "num_classes = 2\n",
        "plot_single_generated_image(generator, latent_dim, num_classes)\n"
      ],
      "metadata": {
        "id": "kI6oLqVhFFrj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}